---
title: "GLMER - Environmental drivers of white plague disease on shallow and mesophotic coral reefs, U.S. Virgin Islands "
author: "Andia Chaves Fonnegra and Bernd Panassiti"
date: "10/11/2017"
output: pdf_document
affiliation: University of the Virgin Islands (US.VI) and Laimburg Research Center (Italy)
abstract: "This document provides the r-code for a series of GLMER's to model  white plague disease  described on the article:"Environmental drivers of white plague disease on shallow and mesophotic coral reefs, U.S. Virgin Islands" Description of the model in terms of data collection, white plague disease, parameter estimations and fitting of the model can be found in Appendix A."
keywords: white plague, coral reefs, glmer, environment
number_sections: yes
    toc: yes
header-includes: \usepackage{graphicx}
email: \email{}
---

```{r setup, include=FALSE, warnings=FALSE}
library(knitr)
opts_knit$set(root.dir='../')                     # definining working directory; or normalizePath('../')
opts_chunk$set(fig.align='center',                # aligns all figures
                echo=TRUE,                        # shows r-code
                message=FALSE,                    # suppresses library outputs
                warnings=FALSE,                   # suppresses library outputs
                tidy=TRUE,  # prevents the source code from running off a pdf page
                dev='pdf')                        # pdf device
```

#Introduction

This document provides the r-code for the glmer's to understand posible interactions of factors afecting the prevalence of white plague disease on St Thomas coral reefs.The model address to the objectives of this study: (1) to evaluate the annual patterns of WP disease, and (2) to identify and quantify the influence of environmental factors on WP disease prevalence. 

#Data
The white plague prevalence data and corresponding environmental and biological variables used in this model can be downloaded from xxxxxxx. The file "whiteplague_working.RData" needs to be placed in a folder called "Data".
"

#Model

##Load libraries and create directory for results
```{r}
# Date format - forms part of names of created files or graphs
today <- format(Sys.time(), "%Y.%m.%d")

#To check data distribution use
require(car)
require(MASS)
#For data inspections we start with a pannel plot also called trellis plots or lattice plots for that we need nlme and lattice libraries
library(nlme)
library(lattice)
library(lme4)
library(coefplot) ## for coefplot2
library(reshape)
library(plyr)
library(gridExtra)
library(emdbook) ## for qchibarsq
library(nlme)
library(ggplot2) # for graphs

library(DHARMa) #to check models residuals

# create directory if not existing
#suppressWarnings(dir.create(file.path(getwd(),"Results/04_Rstan")))

```

## Load data
```{r}
rm(list=ls(all=TRUE))
# Load settings
load(file="data/Whiteplague_workingdata.Rdata")
ls()

#check data
dim(Whiteplague_workingdata)
head(Whiteplague_workingdata)

```

## Data preparation
```{r}

#Response variable=White PLague prevalence

WP<-Whiteplague_workingdata$Plague
ill = Whiteplague_workingdata$ill
Ntot = Whiteplague_workingdata$Ntot

##Selecting Environmental variables from the file already SCALED to use in the model.
#Check in which column scaled environmental variables start.


#Selection of all 11 scaled environmental variables. 
#enviroselection = Whiteplague_workingdata[c(32:42)] 
#enviroselection

# Predictors environmental data
Chlorophyll = Whiteplague_workingdata$Chl_sca
Conductivity = Whiteplague_workingdata$Cond_sca
Density = Whiteplague_workingdata$Den_sca
CTD_Depth = Whiteplague_workingdata$DepCTD_sca    
Oxygen = Whiteplague_workingdata$Oxy_sca
Salinity = Whiteplague_workingdata$Sal_sca    
Turbidity = Whiteplague_workingdata$Turb_sca   
Depth = Whiteplague_workingdata$Depth_tran_sca  
Temperature = Whiteplague_workingdata$Temp_sca   
Max_DHW = Whiteplague_workingdata$DHW_sca 
Rainfall = Whiteplague_workingdata$Rain_sca   

#Selection of Biological variables.
#biological = Whiteplague_workingdata[c(22:27)] 
#biological

# Predictors biological data, all are % of cover in transects. No need to scale?
#Orbicellids = Whiteplague_workingdata$Orbicellids
#Sand = Whiteplague_workingdata$Sand
#Coral = Whiteplague_workingdata$Coral
#Sponges = Whiteplague_workingdata$Sponges    
#Macroalgae = Whiteplague_workingdata$Macroalgae
#Cyanobacteria = Whiteplague_workingdata$Cyanobacteria    

#Time and location factors 
Year=Whiteplague_workingdata$Year
Month=Whiteplague_workingdata$Month_12
Season=Whiteplague_workingdata$Season
Station=Whiteplague_workingdata$Station_name
Reef.Type=Whiteplague_workingdata$Reef.Type
Latitude=Whiteplague_workingdata$Latitude
Longitude=Whiteplague_workingdata$Longitude
Timepoint=Whiteplague_workingdata$TimePoint

Site <- as.numeric(Whiteplague_workingdata$Station_name)
```


#Check probability distribution in the data.
I follow suggestions by: http://ase.tufts.edu/gsc/gradresources/guidetomixedmodelsinr/mixed%20model%20guide.html. The y axis represents the observations and the x axis represents the quantiles modeled by the distribution. 
The solid red line represents a perfect distribution fit and the dashed red lines are the confidence intervals of the perfect distribution fit. 
```{r}
#normality test
#Shapiro-Wilkinson normality test
shapiro.test(WP)
#for producing a normal quantile-quantile plot
qqnorm(WP)
qqline(WP)

# This is to also check if follows a normal distribution. But we need to ad the value of 1 to avoid problems
WP1<-WP+1

qqp(WP, "norm")
# This is to check if follows a LOG normal distribution
qqp(WP, "lnorm")


# qqp requires estimates of the parameters of the negative binomial, Poisson
# and gamma distributions. You can generate estimates using the fitdistr
# function. Save the output and extract the estimates of each parameter as I
# have shown below.
nbinom <- fitdistr(WP1, "negative binomial")
qqp(WP1, "nbinom", size = nbinom$estimate[[1]], mu = nbinom$estimate[[2]])

poisson <- fitdistr(WP1, "poisson")
qqp(WP1, "pois", poisson$estimate)

gamma <- fitdistr(WP1, "gamma")
qqp(WP1, "gamma", shape = gamma$estimate[[1]], rate = gamma$estimate[[2]])

#You want to pick the distribution for which the largest number of observations falls between the dashed lines. In this case, that's the lognormal distribution. 
#Now, armed with the knowledge of which probability distribution fits best, I can try fitting a model.

```


#glmer
Generalized Linear Mixed-Effects Models using Temperature, Rain and Depth as fixed factors
and Month and site as random factors.

from: http://ase.tufts.edu/gsc/gradresources/guidetomixedmodelsinr/mixed%20model%20guide.html

Because the response variable is binary, we will need a generalized linear mixed model with a binomial distribution, and because we have fewer than five random effects, we can use the Laplace approximation.

Strictly speaking, the Laplace approximation is a special case of a parameter estimation method called Gauss-Hermite quadrature (GHQ), with one iteration. GHQ is more accurate than Laplace due to repeated iterations, but becomes less flexible after the first iteration, so you can only use it for one random effect. 

```{r}

#Correct binomial model using total and ill coral colonies
E1 <- glmer(cbind(ill,Ntot-ill) ~ Depth + Turbidity + (1|Month/Site),family="binomial"(link = "logit"))
summary(E1)

#two random effects one temporal month nested within season and one spatial site nested within depth
E2 <- glmer(cbind(ill,Ntot-ill) ~ Depth + Turbidity + (1|Season/Month) + (1|Depth/Site) ,family="binomial"(link = "logit"))
summary(E2)

#Month nested within site. Intercept varying among sites and months within sites
E3 <- glmer(cbind(ill,Ntot-ill) ~ Depth + Turbidity + (1|Site/Month) ,family="binomial"(link = "logit"))
summary(E3)

#combination of effect of mont and site nested within season. works good, still some
E4 <- glmer(cbind(ill,Ntot-ill) ~ Depth + Turbidity + (1|Season/Month:Site) ,family="binomial"(link = "logit"))
summary(E4)

#tests without other random factors does not work
E5 <- glmer(cbind(Ntot,ill) ~  Month + Turbidity + (1|Site/Depth) ,family="binomial"(link = "logit"))
summary(E5)

E6 <- glmer(cbind(ill,Ntot-ill) ~ Depth + Turbidity + (1|Site*Month) ,family="binomial"(link = "logit"))
summary(E6)

E7<- glmer(cbind(ill,Ntot-ill) ~ Depth + Turbidity + (1|Month) ,family="binomial"(link = "logit"))
summary(E7)


E8<-glmer(cbind(ill,Ntot-ill) ~ Depth + (1|Site/Month) + ,family="binomial"(link = "logit"))
summary(E8)

---------

overdisp_fun <- function(E2) {
        ## number of variance parameters in an n-by-n variance-covariance matrix
        vpars <- function(m) {
                nrow(m) * (nrow(m) + 1)/2
        }
        # The next two lines calculate the residual degrees of freedom
        model.df <- sum(sapply(VarCorr(model), vpars)) + length(fixef(model))
        rdf <- nrow(model.frame(model)) - model.df
        # extracts the Pearson residuals
        rp <- residuals(model, type = "pearson")
        Pearson.chisq <- sum(rp^2)
        prat <- Pearson.chisq/rdf
        # Generates a p-value. If less than 0.05, the data are overdispersed.
        pval <- pchisq(Pearson.chisq, df = rdf, lower.tail = FALSE)
        c(chisq = Pearson.chisq, ratio = prat, rdf = rdf, p = pval)
        }


```

#Check model residuals with Dharma

```{r}
#The scaled (quantile) residuals are calculated with the simulateResiduals() function
#What the function does is a) creating n new synthetic datasets by simulating from the fitted model, b) calculates the cumulative distribution of simulated values for each observed value, and c) returning the quantile value that corresponds to the observed value.
simulationOutput <- simulateResiduals(fittedModel = E4, n = 250)

#The calculated residuals are stored in
simulationOutput$scaledResiduals

#For a correctly specified model we would expect a uniform (flat) distribution of the overall residuals uniformity in y direction if we plot against any predictor.

plotSimulatedResiduals(simulationOutput = simulationOutput)

#goodness of fit test
testUniformity(simulationOutput = simulationOutput)

```

#### Spatial autocorrelation of residuals
```{r}
#test Spatial autocorrelation Moran's I

# Problem repeating sites result in 0 distances!
library(dplyr)
df <- data.frame(scaledResiduals=simulationOutput$scaledResiduals,x=Whiteplague_workingdata$Latitude,y=Whiteplague_workingdata$Longitude)

# 1 option - group and mean

dfNew <- as.data.frame(df %>%
  group_by(x, y) %>%
  summarize(scaledResiduals = mean(scaledResiduals, na.rm = TRUE)))

testSpatialAutocorrelation(simulationOutput = dfNew, x = dfNew$x,y = dfNew$y)

# 2. option - random noise
df$x_new <- rnorm(df$x,df$x,0.001)
df$y_new <- rnorm(df$y,df$y,0.001)

testSpatialAutocorrelation(simulationOutput = df, x = df$x_new,y = df$y_new)
```

Both tests indicate that we should reject the null of no spatial auto-correlation. There is spatial autocorrelation.

### Test for residuals temporal autocorrelation
```{r}
# test temporal autocorrelation Durbin-Watson test in the residuals
#Month
testTemporalAutocorrelation(simulationOutput = df, time = Whiteplague_workingdata$Month_12, plot=T)
#Year
testTemporalAutocorrelation(simulationOutput = df, time = Whiteplague_workingdata$Year, plot=T)
#Seasons
testTemporalAutocorrelation(simulationOutput = df, time = Whiteplague_workingdata$Season, plot=T)
#Time points
testTemporalAutocorrelation(simulationOutput = df, time = Whiteplague_workingdata$Time_point, plot=T)
#Random time
#testTemporalAutocorrelation(simulationOutput = df, time = "random")

```

#------------------------------------------------------------------------------------------
#GLMER2 in STAN

#Libraries
```{r}
library(lme4)
#install.packages("devtools")
library(devtools)
#install_github("rmcelreath/glmer2stan")
library(glmer2stan)


```

##Month nested within site. Intercept varying among sites and months within sites
```{r}
df <- data.frame(cbind(ill,Ntot,Depth,Turbidity,Site,Month))
df$healthy <-   df$Ntot-df$ill              
#E3 <- glmer(cbind(ill,Ntot-ill) ~ Depth + Turbidity + (1|Site/Month),data=df,family="binomial"(link = "logit"))
#summary(E3)



df$Site_index <- as.integer(Whiteplague_workingdata$Station_name)
df$Month_index=as.integer(Whiteplague_workingdata$Month_12)
df$Season_index=as.integer(Whiteplague_workingdata$Season)

G1 <- glmer2stan(cbind(ill,healthy) ~ Depth + Turbidity + (1+ Site_index|Month_index) ,data=df,family="binomial")

G1 <- glmer2stan(cbind(ill,healthy) ~ Depth + Turbidity + (1 + Site_index|Month_index) + ( 1+ Month_index|Site_index),data=df,family="binomial")

#Just Month to compare with glmer and brms
G1 <- glmer2stan(cbind(ill,healthy) ~ Depth + Turbidity + (1|Month_index), data=df,family="binomial")


```


# Summary and Check Residuals for glmer2stan Model

```{r}
summary(G1)
stanmer(G1)
i <- varef(G1)
```

#extract posterior

```{r}

str(G1)

```

#Get qqplot of standarized residuals

```{r}
par(mfrow=c(1,2))
predInf <- stanpredict(G1, data=df)$ill
v <- (df$ill/df$Ntot)-predInf$mu
range01 <- function(x){(x-min(x))/(max(x)-min(x))}
resScaled <- range01(v)
gap::qqunif(resScaled,pch = 2, bty = "n", 
        logscale = F, col = "black", cex = 0.6, main = "QQ plot residuals", 
        cex.main = 1)



plot(predInf$mu, v/sd(v), 
ylab="Standardized Residuals", 
xlab="preds", ) 
abline(0, 0) 
par(mfrow=c(1,1))

#predict:

predInf <- stanpredict(G1, data=df)$ill


#difference:

v <- (df$ill/df$Ntot)-predInf$mu


#to scale btw 0 and 1:

range01 <- function(x){(x-min(x))/(max(x)-min(x))}
resScaled <- range01(v)
```

